{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, json\n",
    "from scrapy.http import TextResponse\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import idna\n",
    "from urllib3.exceptions import LocationValueError\n",
    "import time\n",
    "\n",
    "from geonamescache.mappers import country\n",
    "mapper = country(from_key='name', to_key='iso')\n",
    "\n",
    "with open('google.key','r') as f:\n",
    "    APIKEY = f.read()\n",
    "    \n",
    "AMP_BATCHGET_URL = 'https://acceleratedmobilepageurl.googleapis.com/v1/ampUrls:batchGet?key=' + APIKEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = {'User-Agent': 'Mozilla/5.0 (Linux; Android 7.0; SM-G892A Build/NRD90M; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/60.0.3112.107 Mobile Safari/537.36'}\n",
    "\n",
    "def getArticleLinks(url):\n",
    "    \n",
    "    links = []\n",
    "    series = pd.Series(data=links)\n",
    "    symbols = 0\n",
    "    hyphens = 0\n",
    "    domain = getDomain(url)\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(url, headers=USER_AGENT, timeout=4)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return series\n",
    "    except idna.core.IDNAError as e:\n",
    "        return series\n",
    "    except UnicodeError:\n",
    "        return series\n",
    "    except UnicodeEncodeError:\n",
    "        return series\n",
    "    except LocationValueError:\n",
    "        return series\n",
    "    \n",
    "    try:\n",
    "        response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "        c = response.xpath('//a[contains(@href, \"-\")]/@href').extract()\n",
    "    #c = response.xpath('//a/@href').extract()\n",
    "    except UnicodeEncodeError:\n",
    "        return series\n",
    "    \n",
    "    my_regex = r\"^https+://.*\" + re.escape(domain) + r\".*\"\n",
    "    #my_regex1 = r\".*\" + re.escape(domain) + r\"/.*\"\n",
    "    #my_regex2 = r\".*\" + re.escape(domain) + r\"/\\d+.html\"\n",
    "    \n",
    "    for link in c:\n",
    "        hyphens = link.count('-')\n",
    "        symbols = link.count('?') + link.count('#') + link.count('&') + link.count('=')\n",
    "        \n",
    "        #if link has more than 5 hyphens, it is very likely it is a news link\n",
    "        if (hyphens > 5 and symbols < 1):\n",
    "            #if found most likely it has the http(s) in there too\n",
    "            #if (re.match(my_regex,link, re.IGNORECASE)):\n",
    "                if ('http' in link):\n",
    "                    links.append(link)\n",
    "                else:\n",
    "                    links.append(\"http://\" + domain + '/' + link)\n",
    "        \n",
    "        #if (re.search(my_regex2, link, re.IGNORECASE)):\n",
    "        #    print(link)\n",
    "    series = pd.Series(data=links)\n",
    "    series = series.drop_duplicates(keep='first')\n",
    "    \n",
    "    return series\n",
    "\n",
    "def getAMPUrl(link):\n",
    "    c = None\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(link, headers=USER_AGENT, timeout=4)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return c\n",
    "        \n",
    "    response = TextResponse(r.url, body=r.text, encoding='utf-8')\n",
    "    c = response.xpath('//link[contains(@rel, \"amphtml\")]/@href').extract()\n",
    "    \n",
    "    return c\n",
    "\n",
    "def getHTTPCode(link):\n",
    "    c = None\n",
    "    \n",
    "    try:    \n",
    "        r = requests.get(link, headers=USER_AGENT, timeout=4)\n",
    "    except requests.exceptions.RequestException as e:  # This is the correct syntax\n",
    "        return c\n",
    "        \n",
    "    return r.status_code\n",
    "\n",
    "def getSampleLinksByCC(df, cc, n):\n",
    "    df_cc = df.loc[df['cc']==cc]\n",
    "    if (len(df_cc) < n):\n",
    "        return df_cc\n",
    "    else:\n",
    "        return df_cc.sample(n=n)\n",
    "    \n",
    "def getDomain(url):\n",
    "    parsed_uri = urlparse(url)\n",
    "    domain = '{uri.netloc}'.format(uri=parsed_uri)\n",
    "    return domain\n",
    "\n",
    "def split_dataframe(df, chunk_size):\n",
    "    \n",
    "    chunks = []\n",
    "    length = len(df)\n",
    "    \n",
    "    while (length > chunk_size):\n",
    "        df_head = df.head(chunk_size)\n",
    "        df = df.tail(length - chunk_size)\n",
    "        length = len(df)\n",
    "        chunks.append(df_head)\n",
    "    \n",
    "    if length < chunk_size:\n",
    "        chunks.append(df.head(length))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def getAMPUrls(df_urls):\n",
    "    urls  = np.array(df_urls['url'].values)      \n",
    "    \n",
    "    HEADERS = {'accept': 'application/json',\n",
    "            'content-type': 'application/json',\n",
    "            'cookie': 'ASP.NET_SessionId=aiggen1ccck0gq141dgq1sip; ASP.NET_SessionId=aiggen1ccck0gq141dgq1sip'\n",
    "          }\n",
    "    \n",
    "    body = {\n",
    "          'lookupStrategy': 'IN_INDEX_DOC',\n",
    "          'urls': urls.tolist()\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(AMP_BATCHGET_URL, data=json.dumps(body), headers=HEADERS)\n",
    "        res = json.loads(r.text)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return None\n",
    "    except json.decoder.JSONDecodeError as e:\n",
    "        return None\n",
    "    \n",
    "    return res\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = pd.read_csv('data/mediasources.csv', encoding='latin1')\n",
    "df_domains['cc'] = df_domains.apply(lambda x: mapper(x['country']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = df_domains.loc[:,['name', 'link', 'cc']]\n",
    "df_domains = df_domains.dropna()\n",
    "df_domains = df_domains.drop_duplicates()\n",
    "df_domains.to_csv('data/domains.csv', sep='|', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get links from ABYZNEWS for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_links = pd.DataFrame(columns=['cc','name','domain','url','ori_amp_url', 'amp_viewer_url', 'amp_cdn_url'])\n",
    "df_links.to_csv('data/links.csv', sep=',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains = pd.read_csv('data/domains2.csv', sep='|', encoding='utf-8')\n",
    "df_domains = df_domains.drop_duplicates()\n",
    "df_domains['domain'] = df_domains.apply(lambda x: getDomain(x['link']), axis=1)\n",
    "#df_domains = df_domains.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28155"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_domains = df_domains.drop_duplicates()\n",
    "#len(df_domains.domain.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=10\n",
    "domains = []\n",
    "\n",
    "f = open(\"data/links3.csv\", \"a\")\n",
    "#f.write(\"cc|name|domain|url\\n\")\n",
    "\n",
    "for index, row in df_domains.iterrows():\n",
    "    url = row['link']\n",
    "    \n",
    "    symbols = url.count('?') + url.count('#') + url.count('&') + url.count('=') + url.count(',') + url.count(';') \n",
    "    \n",
    "    if (symbols > 0):\n",
    "        continue\n",
    "    \n",
    "    links = getArticleLinks(url)\n",
    "    domain = row['domain']\n",
    "            \n",
    "    if (domain not in domains):\n",
    "        domains.append(domain)\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    if (links is None or links.size < 1):\n",
    "        continue\n",
    "  \n",
    "    if (len(links) <= sample_size):\n",
    "        sample_links = links.head(len(links))\n",
    "    else:\n",
    "        sample_links = links.sample(sample_size)\n",
    "    \n",
    "    #print(\"Domain=\"+domain + \"   Length=\" + str(len(sample_links)))\n",
    "        \n",
    "    for index, url in sample_links.iteritems():\n",
    "        \n",
    "        f.write(\"\\\"\" + str(row['cc']) + \"\\\"|\\\"\" + \n",
    "                str(row['name']) + \"\\\"|\\\"\" +\n",
    "                str(domain) + \"\\\"|\\\"\" + \n",
    "                str(url) + \"\\\"\\n\"\n",
    "               )\n",
    "    \n",
    "    f.flush()    \n",
    "f.close()\n",
    "\n",
    "#         df_links = df_links.append({'name': row['name'], \n",
    "#                              'cc': row['cc'],\n",
    "#                              'domain': domain,\n",
    "#                              'url': url,\n",
    "#                              'ori_amp_url': None, \n",
    "#                              'amp_viewer_url': None, \n",
    "#                              'amp_cdn_url' : None}, \n",
    "#                             ignore_index=True)\n",
    "    \n",
    "#     df_links.to_csv('data/links.csv', mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_domains.loc[df_domains.cc == 'CA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/links2.csv\", sep=\";\", encoding='utf-8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_dataframe(df, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = getAMPUrls(chunks[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "f = open(\"data/amp_urls1.csv\", \"a\")\n",
    "#f.write(\"originalUrl;ampUrl;cdnAmpUrl\\n\")\n",
    "\n",
    "for chunk in chunks:\n",
    "    res = getAMPUrls(chunk)\n",
    "    \n",
    "    if (res is None):\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        if (res['error']['code'] == 429):\n",
    "            print(\"Resources exceeded\")\n",
    "            time.sleep(100)\n",
    "            res = getAMPUrls(chunk)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    urls=[]\n",
    "    errors=[]\n",
    "    try:\n",
    "        urls = res['ampUrls']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        errors = res['urlErrors']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    merged = urls + errors\n",
    "\n",
    "    for u in merged:\n",
    "        ori = u['originalUrl']\n",
    "        amp = \"\"\n",
    "        cdn = \"\"\n",
    "\n",
    "        try: \n",
    "            amp = u['ampUrl']\n",
    "            cdn = u['cdnAmpUrl']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        f.write(\"\\\"\" + ori + \"\\\";\\\"\" + amp + \"\\\";\\\"\" + cdn + \"\\\"\\n\")\n",
    "    \n",
    "    f.flush()    \n",
    "    \n",
    "#     count = count + 1\n",
    "#     if (count == 15):\n",
    "#         print(\"count=\" + str(count))\n",
    "#         time.sleep(110)\n",
    "#         count = 1\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = json.loads(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['error']['code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/links2.csv\", sep=\";\", encoding='utf8', codec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = df.url.values[162].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'urllib3' has no attribute 'unquote'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-28bfa241ccb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0murllib3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'urllib3' has no attribute 'unquote'"
     ]
    }
   ],
   "source": [
    "import urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
